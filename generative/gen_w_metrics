"""
Advanced Multi-Modal Emotion Detection - Validation Focused Version
With robust model saving and comprehensive validation metrics
"""

import sys
import os
import pandas as pd
import numpy as np
from tqdm import tqdm
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score,
                            accuracy_score, f1_score, precision_score, recall_score, 
                            confusion_matrix)
import glob
import random
import json
import copy
import warnings
warnings.filterwarnings('ignore')

# ============ BUILT-IN CONFIGURATION ============
DATA_ROOT = "/home/dipanjan/rugraj/DIAC-WOZ/"
TRAIN_SPLIT = os.path.join(DATA_ROOT, "train_split_Depression_AVEC2017.csv")
DEV_SPLIT = os.path.join(DATA_ROOT, "dev_split_Depression_AVEC2017.csv")

MODEL_CONFIG = {
    "text_model": "distilbert-base-uncased",
    "hidden_dim": 512,
    "num_heads": 8,
    "num_layers": 6,
    "dropout": 0.15,
    "fusion_type": "dynamic",
    "max_audio_frames": 100,
    "max_video_frames": 100,
    "tcn_layers": 3,
}

TRAINING_CONFIG = {
    "batch_size": 6,
    "learning_rate": 3e-5,
    "epochs": 60,
    "max_text_length": 256,
    "weight_decay": 0.01,
    "grad_clip_norm": 1.0,
    "warmup_ratio": 0.1,
    "lr_scheduler": "onecycle",
    "num_workers": 4,
    "pin_memory": True,
    "early_stopping_patience": 10,
}

MULTITASK_CONFIG = {
    "main_task_weight": 1.0,
    "aux_task_weight": 0.2,
    "contrastive_weight": 0.3,
    "use_contrastive": True,
    "use_auxiliary_tasks": True,
    "contrastive_temperature": 0.07,
    "projection_dim": 128,
}

AUGMENTATION_CONFIG = {
    "enable_training_augmentation": True,
    "audio_noise_prob": 0.3,
    "audio_noise_std": 0.01,
    "video_dropout_prob": 0.3,
    "video_dropout_rate": 0.1,
}

OUTPUT_CONFIG = {
    "output_dir": os.path.join(DATA_ROOT, "trained_models"),
    "model_name": "advanced_multimodal",
    "save_predictions": True,
    "save_best_model": True,  # Can be set to False to disable
    "verbose_logging": True,
}

# Set device
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üöÄ Using device: {DEVICE}")

# ============ DATA LOADING ============
def read_transcript_with_turns(session_folder, participant_only=True):
    """Enhanced transcript reading that preserves turn structure"""
    file_candidate = glob.glob(os.path.join(session_folder, "*_TRANSCRIPT.csv"))
    if not file_candidate:
        return "", []

    fp = file_candidate[0]
    try:
        dfh = pd.read_csv(fp, sep='\t', dtype=str, quoting=3)
        if 'speaker' in dfh.columns and 'utterance' in dfh.columns:
            if participant_only:
                part_texts = dfh[dfh['speaker'].str.lower() != 'ellie']['utterance'].fillna('')
                turns = part_texts.tolist()
            else:
                turns = dfh['utterance'].fillna('').tolist()
            full_text = " ".join(turns)
            return full_text, turns[:10]
    except Exception:
        pass

    try:
        df = pd.read_csv(fp, sep='\t', header=None, quoting=3, dtype=str, engine='python')
        if df.shape[1] >= 3:
            text_col = df.iloc[:, -1].fillna('').astype(str)
            full_text = " ".join(text_col.tolist())
            return full_text, text_col.tolist()[:10]
    except Exception:
        pass

    return "", []


def load_temporal_covarep_features(session_folder, max_frames=100):
    """Load temporal audio features"""
    fp = glob.glob(os.path.join(session_folder, "*_COVAREP.csv"))
    if not fp:
        return None

    try:
        df = pd.read_csv(fp[0], index_col=False)

        # Handle VUV column
        col_lower = [c.lower() for c in df.columns]
        vuv_idx = None
        for i, c in enumerate(col_lower):
            if c == 'vuv' or 'vuv' in c:
                vuv_idx = df.columns[i]
                break

        if vuv_idx is not None:
            mask = df[vuv_idx] == 0
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            numeric_cols = [c for c in numeric_cols if c != vuv_idx]
            df.loc[mask, numeric_cols] = 0.0

        numeric = df.select_dtypes(include=[np.number]).to_numpy(dtype=np.float32)
        if numeric.size == 0:
            return None

        # Sample frames uniformly if too many
        if len(numeric) > max_frames:
            indices = np.linspace(0, len(numeric)-1, max_frames, dtype=int)
            numeric = numeric[indices]

        # Pad if too few frames
        if len(numeric) < max_frames:
            padding = np.zeros((max_frames - len(numeric), numeric.shape[1]), dtype=np.float32)
            numeric = np.vstack([numeric, padding])

        numeric = np.nan_to_num(numeric, nan=0.0, posinf=1e6, neginf=-1e6)
        return numeric
    except Exception as e:
        print(f"Error loading COVAREP: {e}")
        return None


def load_temporal_clnf_aus(session_folder, max_frames=100):
    """Load temporal facial action units"""
    fp = glob.glob(os.path.join(session_folder, "*_CLNF_AUs.csv"))
    if not fp:
        return None

    try:
        df = pd.read_csv(fp[0], index_col=False)

        cols = [c for c in df.columns if str(c).endswith('_r')]
        if not cols:
            cols = df.select_dtypes(include=[np.number]).columns.tolist()

        if not cols:
            return None

        numeric = df[cols].to_numpy(dtype=np.float32)
        if numeric.size == 0:
            return None

        # Sample frames uniformly if too many
        if len(numeric) > max_frames:
            indices = np.linspace(0, len(numeric)-1, max_frames, dtype=int)
            numeric = numeric[indices]

        # Pad if too few frames
        if len(numeric) < max_frames:
            padding = np.zeros((max_frames - len(numeric), numeric.shape[1]), dtype=np.float32)
            numeric = np.vstack([numeric, padding])

        numeric = np.nan_to_num(numeric, nan=0.0, posinf=1e6, neginf=-1e6)
        return numeric
    except Exception as e:
        print(f"Error loading CLNF: {e}")
        return None


def read_split_csv(split_csv):
    """Read split CSV"""
    df = pd.read_csv(split_csv)
    possible_id_cols = [c for c in df.columns if any(x in c.lower() for x in ['participant','id','file'])]
    pid_col = possible_id_cols[0] if possible_id_cols else df.columns[0]
    phq_cols = [c for c in df.columns if 'phq' in c.lower()]
    phq_col = phq_cols[0] if phq_cols else None

    out = pd.DataFrame()
    out['participant'] = df[pid_col].astype(str)
    if phq_col is not None:
        out['phq8'] = pd.to_numeric(df[phq_col], errors='coerce')
    else:
        out['phq8'] = np.nan
    out['session_folder'] = out['participant'].apply(lambda x: f"{x}_P" if not str(x).endswith('_P') else x)
    return out


def build_enhanced_dataset_index(split_df):
    max_audio_frames = MODEL_CONFIG["max_audio_frames"]
    max_video_frames = MODEL_CONFIG["max_video_frames"]
    
    rows = []
    stats = {
        'total': 0,
        'valid_phq8': 0,
        'valid_audio': 0,
        'valid_video': 0,
        'emotion_dist': {0: 0, 1: 0}
    }
    
    for _, r in tqdm(split_df.iterrows(), total=len(split_df), desc="Building index"):
        stats['total'] += 1
        sfolder = os.path.join(DATA_ROOT, str(r['session_folder']))
        
        if not os.path.isdir(sfolder):
            candidates = glob.glob(os.path.join(DATA_ROOT, f"{r['participant']}*_P")) + \
                        glob.glob(os.path.join(DATA_ROOT, f"*{r['participant']}*_P"))
            if candidates:
                sfolder = candidates[0]
            else:
                continue
        
        phq8 = r['phq8'] if not pd.isna(r['phq8']) else None
        if phq8 is not None:
            stats['valid_phq8'] += 1
        
        transcript, turns = read_transcript_with_turns(sfolder, participant_only=True)
        audio_temporal = load_temporal_covarep_features(sfolder, max_audio_frames)
        video_temporal = load_temporal_clnf_aus(sfolder, max_video_frames)
        
        if audio_temporal is not None:
            stats['valid_audio'] += 1
        if video_temporal is not None:
            stats['valid_video'] += 1
        
        if audio_temporal is None and video_temporal is None:
            continue
            
        if audio_temporal is None:
            audio_temporal = np.zeros((max_audio_frames, 10), dtype=np.float32)
        if video_temporal is None:
            video_temporal = np.zeros((max_video_frames, 10), dtype=np.float32)
        
        emotion_category = 0 if phq8 is not None and phq8 < 8 else 1
        stats['emotion_dist'][emotion_category] += 1
        
        rows.append({
            'session_folder': sfolder,
            'transcript': transcript,
            'turns': turns,
            'audio_temporal': audio_temporal.astype(np.float32),
            'video_temporal': video_temporal.astype(np.float32),
            'phq8': phq8,
            'emotion_category': emotion_category,
            'participant': r['participant']
        })
    
    print(f"\nDataset Statistics:")
    print(f"Total samples processed: {stats['total']}")
    print(f"Valid samples created: {len(rows)}")
    print(f"Samples with valid PHQ8: {stats['valid_phq8']}")
    print(f"Samples with audio: {stats['valid_audio']}")
    print(f"Samples with video: {stats['valid_video']}")
    print("\nEmotion Distribution:")
    total = sum(stats['emotion_dist'].values())
    if total > 0:
        for label, count in stats['emotion_dist'].items():
            print(f"Class {label}: {count} ({100*count/total:.1f}%)")
    
    return rows


# ============ DATASET CLASS ============
class ConfigurableDAICWozDataset(Dataset):
    def __init__(self, rows, tokenizer, is_training=False):
        self.rows = rows
        self.tokenizer = tokenizer
        self.is_training = is_training
        self.max_len = TRAINING_CONFIG["max_text_length"]
        self.max_audio_frames = MODEL_CONFIG["max_audio_frames"]
        self.max_video_frames = MODEL_CONFIG["max_video_frames"]

    def __len__(self):
        return len(self.rows)

    def _augment_audio(self, audio_features):
        if (self.is_training and
            AUGMENTATION_CONFIG["enable_training_augmentation"] and
            random.random() < AUGMENTATION_CONFIG["audio_noise_prob"]):
            noise = np.random.normal(0, AUGMENTATION_CONFIG["audio_noise_std"],
                                   audio_features.shape).astype(np.float32)
            audio_features = audio_features + noise
        return audio_features

    def _augment_video(self, video_features):
        if (self.is_training and
            AUGMENTATION_CONFIG["enable_training_augmentation"] and
            random.random() < AUGMENTATION_CONFIG["video_dropout_prob"]):
            mask = np.random.random(video_features.shape) > AUGMENTATION_CONFIG["video_dropout_rate"]
            video_features = video_features * mask
        return video_features

    def __getitem__(self, idx):
        r = self.rows[idx]

        encoding = self.tokenizer(
            r['transcript'] if r['transcript'] else " ",
            padding='max_length',
            truncation=True,
            max_length=self.max_len,
            return_tensors='pt'
        )

        item = {k: v.squeeze(0) for k, v in encoding.items()}

        audio_temp = self._augment_audio(r['audio_temporal'].copy())
        video_temp = self._augment_video(r['video_temporal'].copy())

        item['audio_temporal'] = torch.tensor(audio_temp, dtype=torch.float)
        item['video_temporal'] = torch.tensor(video_temp, dtype=torch.float)
        item['phq8'] = torch.tensor(r['phq8'] if r['phq8'] is not None else np.nan, dtype=torch.float)
        item['emotion_category'] = torch.tensor(r['emotion_category'], dtype=torch.long)
        item['participant'] = r['participant']

        return item


# ============ MODEL COMPONENTS ============
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=1000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]


class TemporalConvNet(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        num_layers = MODEL_CONFIG["tcn_layers"]
        self.convs = nn.ModuleList()
        self.norms = nn.ModuleList()

        for i in range(num_layers):
            dilation = 2 ** i
            conv = nn.Conv1d(
                input_dim if i == 0 else hidden_dim,
                hidden_dim,
                kernel_size=3,
                dilation=dilation,
                padding=dilation
            )
            norm = nn.BatchNorm1d(hidden_dim)
            self.convs.append(conv)
            self.norms.append(norm)

        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(MODEL_CONFIG["dropout"])

    def forward(self, x):
        x = x.transpose(1, 2)

        for conv, norm in zip(self.convs, self.norms):
            residual = x
            x = conv(x)
            x = norm(x)
            x = self.activation(x)
            x = self.dropout(x)

            if residual.size(1) == x.size(1):
                x = x + residual

        return x.transpose(1, 2)


class CrossModalAttention(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        num_heads = MODEL_CONFIG["num_heads"]
        dropout = MODEL_CONFIG["dropout"]

        self.multihead_attn = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 4, hidden_dim)
        )
        self.dropout = nn.Dropout(dropout)

    def forward(self, query, key, value, key_padding_mask=None):
        attn_out, _ = self.multihead_attn(query, key, value, key_padding_mask=key_padding_mask)
        query = self.norm1(query + self.dropout(attn_out))

        ffn_out = self.ffn(query)
        query = self.norm2(query + self.dropout(ffn_out))

        return query


class DynamicFusion(nn.Module):
    def __init__(self, input_dims, output_dim):
        super().__init__()
        self.modality_weights = nn.Parameter(torch.ones(len(input_dims)))
        self.fusion_layers = nn.ModuleList([
            nn.Linear(dim, output_dim) for dim in input_dims
        ])
        self.final_projection = nn.Linear(output_dim, output_dim)
        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(MODEL_CONFIG["dropout"])

    def forward(self, modality_features):
        weights = F.softmax(self.modality_weights, dim=0)

        projected = []
        for i, (features, layer) in enumerate(zip(modality_features, self.fusion_layers)):
            proj = layer(features) * weights[i]
            projected.append(proj)

        fused = torch.stack(projected, dim=0).sum(dim=0)
        fused = self.final_projection(self.dropout(self.activation(fused)))

        return fused


# ============ MAIN MODEL ============
class ConfigurableMultiModalModel(nn.Module):
    def __init__(self, audio_dim, video_dim):
        super().__init__()
        hidden_dim = MODEL_CONFIG["hidden_dim"]
        num_layers = MODEL_CONFIG["num_layers"]
        text_model = MODEL_CONFIG["text_model"]

        self.text_encoder = AutoModel.from_pretrained(text_model)
        text_dim = self.text_encoder.config.hidden_size
        self.text_projection = nn.Linear(text_dim, hidden_dim)

        self.audio_tcn = TemporalConvNet(audio_dim, hidden_dim)
        self.video_tcn = TemporalConvNet(video_dim, hidden_dim)

        self.pos_encoding = PositionalEncoding(hidden_dim)

        self.cross_modal_layers = nn.ModuleList([
            CrossModalAttention(hidden_dim) for _ in range(num_layers)
        ])

        fusion_type = MODEL_CONFIG["fusion_type"]
        if fusion_type == "dynamic":
            self.fusion = DynamicFusion([hidden_dim, hidden_dim, hidden_dim], hidden_dim)
        else:
            self.fusion = nn.Linear(hidden_dim * 3, hidden_dim)

        self.depression_regressor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(MODEL_CONFIG["dropout"]),
            nn.Linear(hidden_dim // 2, 1)
        )

        if MULTITASK_CONFIG["use_auxiliary_tasks"]:
            self.emotion_classifier = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(MODEL_CONFIG["dropout"]),
                nn.Linear(hidden_dim // 2, 2)
            )

        if MULTITASK_CONFIG["use_contrastive"]:
            proj_dim = MULTITASK_CONFIG["projection_dim"]
            self.contrastive_projection = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Linear(hidden_dim // 2, proj_dim)
            )

    def forward(self, input_ids, attention_mask, audio_temporal, video_temporal):
        hidden_dim = MODEL_CONFIG["hidden_dim"]

        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)
        text_features = text_outputs.last_hidden_state
        text_pooled = text_features[:, 0, :]
        text_encoded = self.text_projection(text_pooled)

        audio_encoded = self.audio_tcn(audio_temporal)
        audio_pooled = audio_encoded.mean(dim=1)

        video_encoded = self.video_tcn(video_temporal)
        video_pooled = video_encoded.mean(dim=1)

        text_seq = text_encoded.unsqueeze(1)
        audio_seq = audio_pooled.unsqueeze(1)
        video_seq = video_pooled.unsqueeze(1)

        for layer in self.cross_modal_layers[:3]:
            text_seq = layer(text_seq, torch.cat([audio_seq, video_seq], dim=1),
                           torch.cat([audio_seq, video_seq], dim=1))

        text_final = text_seq.squeeze(1)

        fusion_type = MODEL_CONFIG["fusion_type"]
        if fusion_type == "dynamic":
            fused_features = self.fusion([text_final, audio_pooled, video_pooled])
        else:
            fused_features = self.fusion(torch.cat([text_final, audio_pooled, video_pooled], dim=1))

        depression_pred = self.depression_regressor(fused_features).squeeze(1)

        outputs = {'depression_pred': depression_pred}

        if MULTITASK_CONFIG["use_auxiliary_tasks"] and hasattr(self, 'emotion_classifier'):
            outputs['emotion_class_pred'] = self.emotion_classifier(fused_features)

        if MULTITASK_CONFIG["use_contrastive"] and hasattr(self, 'contrastive_projection'):
            contrastive_emb = self.contrastive_projection(fused_features)
            outputs['contrastive_emb'] = F.normalize(contrastive_emb, p=2, dim=1)

        return outputs


# ============ LOSS FUNCTIONS ============
class ContrastiveLoss(nn.Module):
    def __init__(self):
        super().__init__()
        self.temperature = MULTITASK_CONFIG["contrastive_temperature"]

    def forward(self, embeddings, labels):
        batch_size = embeddings.size(0)

        similarity_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature

        labels = labels.unsqueeze(1)
        mask = torch.eq(labels, labels.T).float()

        mask = mask - torch.eye(batch_size, device=mask.device)

        exp_sim = torch.exp(similarity_matrix)
        exp_sim = exp_sim * (1 - torch.eye(batch_size, device=exp_sim.device))

        positive_pairs = exp_sim * mask
        negative_pairs = exp_sim.sum(dim=1, keepdim=True)

        loss = -torch.log(positive_pairs.sum(dim=1) / negative_pairs.squeeze(1))
        loss = loss[~torch.isnan(loss)]

        return loss.mean() if len(loss) > 0 else torch.tensor(0.0, device=embeddings.device)


# ============ COMPREHENSIVE EVALUATION ============
def evaluate_comprehensive(model, loader, device):
    """Comprehensive validation evaluation with all metrics"""
    model.eval()
    
    all_preds = []
    all_labels = []
    all_participants = []
    all_emotion_preds = []
    all_emotion_labels = []
    
    with torch.no_grad():
        for batch in tqdm(loader, desc="Evaluating", leave=False):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            audio_temporal = batch['audio_temporal'].to(device)
            video_temporal = batch['video_temporal'].to(device)
            phq8 = batch['phq8']
            emotion_category = batch['emotion_category'].to(device)
            
            audio_temporal = torch.nan_to_num(audio_temporal, nan=0.0, posinf=1e6, neginf=-1e6)
            video_temporal = torch.nan_to_num(video_temporal, nan=0.0, posinf=1e6, neginf=-1e6)
            
            outputs = model(input_ids, attention_mask, audio_temporal, video_temporal)
            
            valid_mask = ~torch.isnan(phq8)
            if valid_mask.sum() > 0:
                all_preds.extend(outputs['depression_pred'][valid_mask].cpu().numpy().tolist())
                all_labels.extend(phq8[valid_mask].numpy().tolist())
                all_participants.extend([batch['participant'][i] for i in range(len(batch['participant'])) if valid_mask[i]])
            
            if 'emotion_class_pred' in outputs:
                all_emotion_preds.extend(outputs['emotion_class_pred'].argmax(dim=1).cpu().numpy().tolist())
                all_emotion_labels.extend(emotion_category.cpu().numpy().tolist())
    
    if len(all_preds) == 0:
        return None
    
    preds_arr = np.array(all_preds)
    labels_arr = np.array(all_labels)
    
    # Regression metrics
    mse = mean_squared_error(labels_arr, preds_arr)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(labels_arr, preds_arr)
    r2 = r2_score(labels_arr, preds_arr)
    
    # Binary classification metrics (PHQ8 >= 10 = depressed)
    true_binary = (labels_arr >= 10).astype(int)
    pred_binary = (preds_arr >= 10).astype(int)
    
    accuracy = accuracy_score(true_binary, pred_binary)
    f1 = f1_score(true_binary, pred_binary, zero_division=0)
    precision = precision_score(true_binary, pred_binary, zero_division=0)
    recall = recall_score(true_binary, pred_binary, zero_division=0)
    cm = confusion_matrix(true_binary, pred_binary)
    
    # Emotion classification metrics (if available)
    emotion_metrics = None
    if all_emotion_preds and all_emotion_labels:
        emotion_acc = accuracy_score(all_emotion_labels, all_emotion_preds)
        emotion_f1 = f1_score(all_emotion_labels, all_emotion_preds, average='weighted', zero_division=0)
        emotion_metrics = {
            'accuracy': float(emotion_acc),
            'f1': float(emotion_f1)
        }
    
    # Results dataframe
    results_df = pd.DataFrame({
        'participant': all_participants,
        'true_phq8': labels_arr,
        'pred_phq8': preds_arr,
        'error': np.abs(labels_arr - preds_arr),
        'true_depressed': true_binary,
        'pred_depressed': pred_binary
    })
    
    metrics = {
        'num_samples': len(all_preds),
        'rmse': float(rmse),
        'mae': float(mae),
        'mse': float(mse),
        'r2': float(r2),
        'accuracy': float(accuracy),
        'f1': float(f1),
        'precision': float(precision),
        'recall': float(recall),
        'confusion_matrix': cm.tolist(),
        'emotion_metrics': emotion_metrics,
        'results_df': results_df
    }
    
    return metrics


# ============ TRAINING FUNCTION ============
def train_configurable_model(model, train_loader, val_loader, device, save_model=True):
    model.to(device)

    epochs = TRAINING_CONFIG["epochs"]
    lr = TRAINING_CONFIG["learning_rate"]
    weight_decay = TRAINING_CONFIG["weight_decay"]
    grad_clip = TRAINING_CONFIG["grad_clip_norm"]
    patience = TRAINING_CONFIG["early_stopping_patience"]

    mse_loss = nn.MSELoss()
    ce_loss = nn.CrossEntropyLoss()
    contrastive_loss = ContrastiveLoss() if MULTITASK_CONFIG["use_contrastive"] else None

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)

    total_steps = len(train_loader) * epochs
    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=lr, total_steps=total_steps, pct_start=TRAINING_CONFIG["warmup_ratio"]
    )

    history = {
        'train_loss': [],
        'val_rmse': [],
        'val_mae': [],
        'val_r2': [],
        'val_accuracy': [],
        'val_f1': [],
        'lr': []
    }
    
    best_val_rmse = float('inf')
    best_epoch = 0
    epochs_no_improve = 0
    best_model_state = None

    for epoch in range(epochs):
        # Training phase
        model.train()
        total_loss = 0.0
        n_samples = 0

        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]")
        for batch in train_pbar:
            optimizer.zero_grad()

            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            audio_temporal = batch['audio_temporal'].to(device)
            video_temporal = batch['video_temporal'].to(device)
            phq8 = batch['phq8'].to(device)
            emotion_category = batch['emotion_category'].to(device)

            audio_temporal = torch.nan_to_num(audio_temporal, nan=0.0, posinf=1e6, neginf=-1e6)
            video_temporal = torch.nan_to_num(video_temporal, nan=0.0, posinf=1e6, neginf=-1e6)

            outputs = model(input_ids, attention_mask, audio_temporal, video_temporal)

            valid_mask = ~torch.isnan(phq8)
            if valid_mask.sum() == 0:
                continue

            main_loss = mse_loss(outputs['depression_pred'][valid_mask], phq8[valid_mask])
            total_batch_loss = main_loss * MULTITASK_CONFIG["main_task_weight"]

            if MULTITASK_CONFIG["use_auxiliary_tasks"] and 'emotion_class_pred' in outputs:
                aux_loss = ce_loss(outputs['emotion_class_pred'], emotion_category)
                total_batch_loss += aux_loss * MULTITASK_CONFIG["aux_task_weight"]

            if MULTITASK_CONFIG["use_contrastive"] and 'contrastive_emb' in outputs:
                cont_loss = contrastive_loss(outputs['contrastive_emb'], emotion_category)
                total_batch_loss += cont_loss * MULTITASK_CONFIG["contrastive_weight"]

            total_batch_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
            optimizer.step()
            scheduler.step()

            batch_size = valid_mask.sum().item()
            total_loss += total_batch_loss.item() * batch_size
            n_samples += batch_size

            train_pbar.set_postfix({'loss': f"{total_batch_loss.item():.4f}"})

        avg_train_loss = total_loss / n_samples if n_samples > 0 else 0

        # Validation phase
        val_metrics = evaluate_comprehensive(model, val_loader, device)

        if val_metrics is None:
            print("WARNING: No valid validation samples")
            continue

        # Update scheduler based on validation loss
        current_lr = optimizer.param_groups[0]['lr']

        # Store history
        history['train_loss'].append(avg_train_loss)
        history['val_rmse'].append(val_metrics['rmse'])
        history['val_mae'].append(val_metrics['mae'])
        history['val_r2'].append(val_metrics['r2'])
        history['val_accuracy'].append(val_metrics['accuracy'])
        history['val_f1'].append(val_metrics['f1'])
        history['lr'].append(current_lr)

        # Print comprehensive metrics
        print(f"\n{'='*70}")
        print(f"Epoch {epoch+1}/{epochs}")
        print(f"{'='*70}")
        print(f"Train Loss: {avg_train_loss:.4f}")
        print(f"\nüìä Regression Metrics (PHQ-8 Prediction):")
        print(f"  RMSE:      {val_metrics['rmse']:.4f}")
        print(f"  MAE:       {val_metrics['mae']:.4f}")
        print(f"  R¬≤:        {val_metrics['r2']:.4f}")
        print(f"  MSE:       {val_metrics['mse']:.4f}")
        print(f"\nüéØ Classification Metrics (Depression Detection, PHQ8 >= 10):")
        print(f"  Accuracy:  {val_metrics['accuracy']:.4f}")
        print(f"  F1-Score:  {val_metrics['f1']:.4f}")
        print(f"  Precision: {val_metrics['precision']:.4f}")
        print(f"  Recall:    {val_metrics['recall']:.4f}")
        print(f"\nüìà Confusion Matrix:")
        print(f"  {val_metrics['confusion_matrix']}")
        
        if val_metrics['emotion_metrics']:
            print(f"\nüòä Emotion Classification Metrics:")
            print(f"  Accuracy:  {val_metrics['emotion_metrics']['accuracy']:.4f}")
            print(f"  F1-Score:  {val_metrics['emotion_metrics']['f1']:.4f}")
        
        print(f"\n‚öôÔ∏è  Learning Rate: {current_lr:.2e}")

        # Save best model
        if val_metrics['rmse'] < best_val_rmse:
            best_val_rmse = val_metrics['rmse']
            best_epoch = epoch + 1
            epochs_no_improve = 0
            
            # Store in memory
            best_model_state = copy.deepcopy(model.state_dict())
            
            # Try to save to disk
            if save_model:
                out_dir = OUTPUT_CONFIG["output_dir"]
                try:
                    os.makedirs(out_dir, exist_ok=True)
                    
                    # Atomic write
                    temp_path = os.path.join(out_dir, f"{OUTPUT_CONFIG['model_name']}_best_temp.pt")
                    final_path = os.path.join(out_dir, f"{OUTPUT_CONFIG['model_name']}_best.pt")
                    
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'val_metrics': {k: v for k, v in val_metrics.items() if k != 'results_df'}
                    }, temp_path)
                    
                    import shutil
                    shutil.move(temp_path, final_path)
                    print(f"\n  ‚úì Model saved to disk: {final_path}")
                    
                except Exception as e:
                    print(f"\n  ‚ö† Warning: Could not save model to disk: {e}")
                    print(f"  ‚Ñπ Model state kept in memory only")
            
            # Save predictions
            if OUTPUT_CONFIG["save_predictions"]:
                try:
                    pred_path = os.path.join(out_dir, f"{OUTPUT_CONFIG['model_name']}_best_predictions.csv")
                    val_metrics['results_df'].to_csv(pred_path, index=False)
                    print(f"  ‚úì Predictions saved: {pred_path}")
                except Exception as e:
                    print(f"  ‚ö† Warning: Could not save predictions: {e}")
            
            print(f"\n  ‚úÖ NEW BEST MODEL! (RMSE: {best_val_rmse:.4f})")
        else:
            epochs_no_improve += 1
            print(f"\n  No improvement ({epochs_no_improve}/{patience})")

        if epochs_no_improve >= patience:
            print(f"\n{'='*70}")
            print(f"Early stopping triggered at epoch {epoch+1}")
            print(f"Best validation RMSE: {best_val_rmse:.4f} at epoch {best_epoch}")
            print(f"{'='*70}")
            break

    # Load best model from memory
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print("\n‚úì Best model loaded from memory")

    return history, best_val_rmse


# ============ MAIN EXECUTION ============
def main():
    print("\n" + "="*70)
    print("üöÄ Advanced Multi-Modal Emotion Detection")
    print("   Validation-Focused Training")
    print("="*70)

    # Check disk space
    try:
        import shutil
        total, used, free = shutil.disk_usage(DATA_ROOT)
        free_gb = free // (2**30)
        print(f"\nüíæ Disk space available: {free_gb} GB")
        if free_gb < 1:
            print("‚ö† WARNING: Low disk space! Model saving may fail.")
            save_model_flag = False
        else:
            save_model_flag = OUTPUT_CONFIG["save_best_model"]
    except:
        save_model_flag = OUTPUT_CONFIG["save_best_model"]

    # Load data
    print("\nüìä Loading data...")
    train_df = read_split_csv(TRAIN_SPLIT)
    dev_df = read_split_csv(DEV_SPLIT)
    print(f"  Train samples: {len(train_df)}, Dev samples: {len(dev_df)}")

    # Build dataset
    print("\nüîß Building dataset indices...")
    train_rows = build_enhanced_dataset_index(train_df)
    dev_rows = build_enhanced_dataset_index(dev_df)

    if not train_rows or not dev_rows:
        print("‚ùå No valid data found!")
        return

    print(f"  Valid samples - Train: {len(train_rows)}, Dev: {len(dev_rows)}")

    # Get feature dimensions
    audio_dim = train_rows[0]['audio_temporal'].shape[1]
    video_dim = train_rows[0]['video_temporal'].shape[1]
    print(f"  Feature dimensions - Audio: {audio_dim}, Video: {video_dim}")

    # Initialize tokenizer and datasets
    print("\nüî§ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG["text_model"])
    train_dataset = ConfigurableDAICWozDataset(train_rows, tokenizer, is_training=True)
    dev_dataset = ConfigurableDAICWozDataset(dev_rows, tokenizer, is_training=False)

    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=TRAINING_CONFIG["batch_size"],
        shuffle=True,
        num_workers=TRAINING_CONFIG["num_workers"],
        pin_memory=TRAINING_CONFIG["pin_memory"],
        drop_last=True
    )
    dev_loader = DataLoader(
        dev_dataset,
        batch_size=TRAINING_CONFIG["batch_size"],
        shuffle=False,
        num_workers=TRAINING_CONFIG["num_workers"],
        pin_memory=TRAINING_CONFIG["pin_memory"]
    )

    # Initialize model
    print(f"\nüèóÔ∏è  Initializing model...")
    model = ConfigurableMultiModalModel(audio_dim=audio_dim, video_dim=video_dim)
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  Total parameters: {total_params:,}")
    print(f"  Trainable parameters: {trainable_params:,}")
    print(f"  Device: {DEVICE}")

    # Training
    print(f"\nüéØ Starting training...")
    print(f"  Epochs: {TRAINING_CONFIG['epochs']}")
    print(f"  Batch size: {TRAINING_CONFIG['batch_size']}")
    print(f"  Learning rate: {TRAINING_CONFIG['learning_rate']}")
    print(f"  Early stopping patience: {TRAINING_CONFIG['early_stopping_patience']}")
    print(f"  Model saving: {'Enabled' if save_model_flag else 'Disabled (low disk space)'}")
    print()
    
    history, best_rmse = train_configurable_model(model, train_loader, dev_loader, DEVICE, save_model=save_model_flag)

    # Save final results
    print("\nüíæ Saving results...")
    out_dir = OUTPUT_CONFIG["output_dir"]
    os.makedirs(out_dir, exist_ok=True)

    # Save training history
    try:
        history_path = os.path.join(out_dir, f"{OUTPUT_CONFIG['model_name']}_history.csv")
        pd.DataFrame(history).to_csv(history_path, index=False)
        print(f"  ‚úì Training history saved: {history_path}")
    except Exception as e:
        print(f"  ‚ö† Could not save training history: {e}")

    # Final validation evaluation
    print("\nüìà Final validation evaluation...")
    final_metrics = evaluate_comprehensive(model, dev_loader, DEVICE)

    if final_metrics:
        # Print final summary
        print(f"\n{'='*70}")
        print("üéâ TRAINING COMPLETE!")
        print(f"{'='*70}")
        print(f"\nüìä FINAL VALIDATION METRICS:")
        print(f"\nRegression (PHQ-8 Prediction):")
        print(f"  RMSE:      {final_metrics['rmse']:.4f}")
        print(f"  MAE:       {final_metrics['mae']:.4f}")
        print(f"  R¬≤:        {final_metrics['r2']:.4f}")
        print(f"  MSE:       {final_metrics['mse']:.4f}")
        print(f"\nClassification (Depression Detection):")
        print(f"  Accuracy:  {final_metrics['accuracy']:.4f}")
        print(f"  F1-Score:  {final_metrics['f1']:.4f}")
        print(f"  Precision: {final_metrics['precision']:.4f}")
        print(f"  Recall:    {final_metrics['recall']:.4f}")
        print(f"\nConfusion Matrix:")
        print(f"  {final_metrics['confusion_matrix']}")
        
        if final_metrics['emotion_metrics']:
            print(f"\nEmotion Classification:")
            print(f"  Accuracy:  {final_metrics['emotion_metrics']['accuracy']:.4f}")
            print(f"  F1-Score:  {final_metrics['emotion_metrics']['f1']:.4f}")
        
        print(f"\nüìà Training Summary:")
        print(f"  Best RMSE: {best_rmse:.4f}")
        print(f"  Total epochs: {len(history['train_loss'])}")

        # Save final metrics
        try:
            metrics_to_save = {k: v for k, v in final_metrics.items() if k != 'results_df'}
            metrics_path = os.path.join(out_dir, f"{OUTPUT_CONFIG['model_name']}_final_metrics.json")
            with open(metrics_path, 'w') as f:
                json.dump(metrics_to_save, f, indent=2)
            print(f"\n  ‚úì Final metrics saved: {metrics_path}")
        except Exception as e:
            print(f"  ‚ö† Could not save metrics: {e}")

    print(f"\n{'='*70}")
    print("üìÅ Output files:")
    if save_model_flag:
        print(f"  - {OUTPUT_CONFIG['model_name']}_best.pt (if disk write succeeded)")
    print(f"  - {OUTPUT_CONFIG['model_name']}_best_predictions.csv")
    print(f"  - {OUTPUT_CONFIG['model_name']}_history.csv")
    print(f"  - {OUTPUT_CONFIG['model_name']}_final_metrics.json")
    print(f"\nüí° Note: Best model is loaded in memory and ready for use")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()